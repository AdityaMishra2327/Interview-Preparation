## Data Structures and Algorithms (DSA) Complexity Cheatsheet

 Overview

In DSA, the main goal is to solve problems effectively and efficiently. To determine the efficiency of a program, we analyze two types of complexities:

- Time Complexity: Measures how much time our code takes to run.
- Space Complexity: Measures how much memory our code uses.

 Time Complexity

| Complexity         | Notation     | Description                                         | Example                                   |
|--------------------|--------------|-----------------------------------------------------|-------------------------------------------|
| Constant           | O(1)         | Execution time is constant regardless of input size | Accessing an element in an array          |
| Logarithmic        | O(log n)     | Time grows logarithmically as input size increases  | Binary search in a sorted array           |
| Linear             | O(n)         | Time grows linearly with input size                 | Finding an element in an unsorted array   |
| Linearithmic       | O(n log n)   | Time grows as n log n; typical of efficient sorting | Merge Sort, Quick Sort                    |
| Quadratic          | O(n²)        | Time grows quadratically with input size            | Bubble Sort, Selection Sort               |
| Cubic              | O(n³)        | Time grows cubically with input size                | Algorithms with three nested loops        |
| Exponential        | O(2^n)       | Time doubles with each additional element           | Tower of Hanoi problem                    |
| Factorial          | O(n!)        | Time grows factorially with input size              | Generating all permutations               |

---

 Space Complexity

| Complexity         | Notation | Description                                     | Example                               |
|------------------------|--------------|-----------------------------------------------------|-------------------------------------------|
| Constant           | O(1)         | Uses a fixed amount of space regardless of input    | Swapping two numbers                      |
| Linear             | O(n)         | Space grows linearly with input size                | Storing an array of n elements            |
| Logarithmic        | O(log n)     | Space grows logarithmically                          | Recursive call stack in binary search     |
| Quadratic          | O(n²)       | Space grows quadratically                            | Dynamic programming table (2D array)      |

---

 Asymptotic Notation

To compare efficiencies of algorithms, we use asymptotic notation, a mathematical tool that estimates time based on input size without running the code. It focuses on the number of basic operations in the program.

| Notation | Description                                                                                     |
|--------------|-----------------------------------------------------------------------------------------------------|
| Big-O (O)    | Describes the worst-case scenario, providing an upper time bound of an algorithm.               |
| Omega (Ω)    | Describes the best-case scenario, offering a lower time bound of an algorithm.                  |
| Theta (θ)    | Represents the average complexity of an algorithm.                                              |

---
![alt text](https://github.com/yashigupta4623/Interview_Preparation-/blob/main/Data%20Structure/1.%20Complexities/image.png)
---

General Notes :
- Best, Average, Worst Cases: Analyze complexities for different scenarios; for example, Quick Sort has O(n log n) average but O(n²) worst-case.
- Amortized Analysis: Average time per operation over a sequence (e.g., dynamic arrays).

Can Expand the Bandwidth with : (this)[https://leetcode.com/explore/interview/card/cheatsheets/720/resources/4725/]
